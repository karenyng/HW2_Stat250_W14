{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stat 250 HW 2 with Prof. Duncan Temple Lang\n",
      "\n",
      "Author: (Karen) Yin-Yee Ng <karenyng@ucdavis.edu>\n",
      "\n",
      "Github repository: https://github.com/karenyng\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autosave 60\n",
      "\n",
      "# the following two lines allows the generating \n",
      "# a floating table of content \n",
      "# currently does not work with nbviewer online nor pdf \n",
      "# I may fix it someday during my infinite spare time \n",
      "%load_ext nbtoc\n",
      "%nbtoc\n",
      "\n",
      "# call R from python using rpy2 \n",
      "%load_ext rmagic \n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "IPython.notebook.set_autosave_interval(60000)"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Autosaving every 60 seconds\n",
        "The nbtoc extension is already loaded. To reload it, use:\n",
        "  %reload_ext nbtoc\n"
       ]
      },
      {
       "html": [
        "<!-- extracted from https://gist.github.com/magican/5574556 -->\n",
        "<div id=\"toc-wrapper\">\n",
        "    <div class=\"header\">Contents <a href=\"#\" class=\"hide-btn\">[hide]</a></div>\n",
        "    <div id=\"toc\"></div>\n",
        "</div>\n",
        " \n",
        "<style>\n",
        "  #toc {\n",
        "    overflow-y: scroll;\n",
        "    max-height: 300px;\n",
        "  }\n",
        "  #toc-wrapper {\n",
        "    position: fixed; top: 120px; max-width:430px; right: 20px;\n",
        "    border: thin solid rgba(0, 0, 0, 0.38); opacity: .8;\n",
        "    border-radius: 5px; background-color: #fff; padding:10px;\n",
        "    z-index: 100;\n",
        "  }\n",
        "  #toc-wrapper.closed {\n",
        "      min-width: 100px;\n",
        "      width: auto;\n",
        "      transition: width;\n",
        "  }\n",
        "  #toc-wrapper:hover{\n",
        "      opacity:1;\n",
        "  }\n",
        "  #toc-wrapper .header {\n",
        "      font-size:18px; font-weight: bold;\n",
        "  }\n",
        "  #toc-wrapper .hide-btn {\n",
        "      font-size: 14px;\n",
        "  }\n",
        " \n",
        "</style>\n",
        "\n",
        "<style>\n",
        "  ol.nested {\n",
        "    counter-reset: item;\n",
        "    list-style: none;\n",
        "  }\n",
        "  li.nested {\n",
        "        display: block;\n",
        "    }\n",
        "  li.nested:before {\n",
        "        counter-increment: item;\n",
        "        content: counters(item, \".\")\" \";\n",
        "    }\n",
        "</style>\n"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "javascript": [
        "// adapted from https://gist.github.com/magican/5574556\n",
        "\n",
        "function clone_anchor(element) {\n",
        "  // clone link\n",
        "  var h = element.find(\"div.text_cell_render\").find(':header').first();\n",
        "  var a = h.find('a').clone();\n",
        "  var new_a = $(\"<a>\");\n",
        "  new_a.attr(\"href\", a.attr(\"href\"));\n",
        "  // get the text *excluding* the link text, whatever it may be\n",
        "  var hclone = h.clone();\n",
        "  hclone.children().remove();\n",
        "  new_a.text(hclone.text());\n",
        "  return new_a;\n",
        "}\n",
        "\n",
        "function ol_depth(element) {\n",
        "  // get depth of nested ol\n",
        "  var d = 0;\n",
        "  while (element.prop(\"tagName\").toLowerCase() == 'ol') {\n",
        "    d += 1;\n",
        "    element = element.parent();\n",
        "  }\n",
        "  return d;\n",
        "}\n",
        "\n",
        "function table_of_contents(threshold) {\n",
        "  if (threshold === undefined) {\n",
        "    threshold = 4;\n",
        "  }\n",
        "  var cells = IPython.notebook.get_cells();\n",
        "  \n",
        "  var ol = $(\"<ol/>\");\n",
        "  $(\"#toc\").empty().append(ol);\n",
        "  \n",
        "  for (var i=0; i < cells.length; i++) {\n",
        "    var cell = cells[i];\n",
        "    \n",
        "    if (cell.cell_type !== 'heading') continue;\n",
        "    \n",
        "    var level = cell.level;\n",
        "    if (level > threshold) continue;\n",
        "    \n",
        "    var depth = ol_depth(ol);\n",
        "\n",
        "    // walk down levels\n",
        "    for (; depth < level; depth++) {\n",
        "      var new_ol = $(\"<ol/>\");\n",
        "      ol.append(new_ol);\n",
        "      ol = new_ol;\n",
        "    }\n",
        "    // walk up levels\n",
        "    for (; depth > level; depth--) {\n",
        "      ol = ol.parent();\n",
        "    }\n",
        "    //\n",
        "    ol.append(\n",
        "      $(\"<li/>\").append(clone_anchor(cell.element))\n",
        "    );\n",
        "  }\n",
        "\n",
        "  $('#toc-wrapper .header').click(function(){\n",
        "    $('#toc').slideToggle();\n",
        "    $('#toc-wrapper').toggleClass('closed');\n",
        "    if ($('#toc-wrapper').hasClass('closed')){\n",
        "      $('#toc-wrapper .hide-btn').text('[show]');\n",
        "    } else {\n",
        "      $('#toc-wrapper .hide-btn').text('[hide]');\n",
        "    }\n",
        "    return false;\n",
        "  })\n",
        "\n",
        "  $(window).resize(function(){\n",
        "    $('#toc').css({maxHeight: $(window).height() - 200})\n",
        "  })\n",
        "\n",
        "  $(window).trigger('resize')\n",
        "}\n",
        "\n",
        "table_of_contents();\n",
        "\n",
        "\n"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The rmagic extension is already loaded. To reload it, use:\n",
        "  %reload_ext rmagic\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background of this assignment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We parallelize previous methods  for computing statistics of large csv methods with goals of: \n",
      "\n",
      "* comparing different ways of parallelizing the code\n",
      "* examining the speeding up of the code from different ways of parallelism\n",
      "\n",
      "Discussion of the previous methods is available at: \n",
      "http://nbviewer.ipython.org/github/karenyng/HW1_Stat250_Winter14/blob/master/writeup/hw1.ipynb?create=1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Possible difficulties / overhead"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* handling data locality - avoid passing data around different workers \n",
      "* overhead of combining data "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notes for myself - Key concepts of parallelism "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Data parallelism\n",
      "\n",
      "handling multiple inputs concurrently\n",
      "\n",
      "> Task parallelism\n",
      "\n",
      "if tasks are not dependent on each other, then they can be perform simultaneously with different workers\n",
      "\n",
      "> Data locality \n",
      "\n",
      "Be careful about how to fork the data.  Fine grained functions is hard to parallelize well since a lot of communication about the data will decrease the efficiency.\n",
      "Also have to be aware of how the language scope the functions / variables when sending them from master node to worker nodes\n",
      "\n",
      "> Race condition \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Intrinsic limitations - how well can we do?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Amdalh's law  (reference from wikipedia...)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ lim_{P-> \\infty} \\frac{1}{\\frac{1-\\alpha}{P} + \\alpha} = \\frac{1}{\\alpha} $$\n",
      "\n",
      "which denotes the maximum speed up with parallelization of the program, where $\\alpha$ is the portion of the sequential parts of the program that is not speed up with addition of processors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gustafson's law "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ S(P) = \\alpha + P(1-\\alpha) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So during profiling I should find $\\alpha$ in order to know how well I can do."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance metric of how well I am parallelizing my code:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(From Scaling Up Machine Learning - Bekkerman R.) \n",
      "\n",
      "* speedup - ratio of solution time for sequential algorithms vs. its parallel counterpart\n",
      "\n",
      "* efficiency - measures the ratio of speed up to the # of processors\n",
      "\n",
      "* scalability - tracks efficiency as a function of an increasing number of processors  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Method 1: Parallelization in python "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My code originally runs for ~3 mins. I am parallelizing it because I want to learn how to do it in python\n",
      "\n",
      "most of the codes between me and my collaborators are in python (it's a community preference not a personal one). And I have written some code with pandas for reading in csv / text files. \n",
      "\n",
      "I mainly make use of the book \"Python high performance programming\" as my reference for this method."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Profiling my python code to see where the bottleneck ia"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Educated guess: the reading in of files would take the most amount of time ... "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I use the cProfile module from python to profile my code - with less overhead than the default (python based) profile module. \n",
      "\n",
      "    python -m cProfile -o profile.out compute_stat.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "See the summary [1]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pstats\n",
      "p = pstats.Stats(\"../analysis/profile.out\")\n",
      "p.sort_stats('cumulative').print_stats(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fri Jan 31 00:23:16 2014    ../analysis/profile.out\n",
        "\n",
        "         112417 function calls (110042 primitive calls) in 190.794 seconds\n",
        "\n",
        "   Ordered by: cumulative time\n",
        "   List reduced from 1670 to 10 due to restriction <10>\n",
        "\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
        "        1    1.405    1.405  190.794  190.794 compute_stat.py:8(<module>)\n",
        "       81    0.014    0.000  168.823    2.084 /usr/lib/python2.7/dist-packages/pandas/io/parsers.py:286(parser_f)\n",
        "       81    0.036    0.000  168.809    2.084 /usr/lib/python2.7/dist-packages/pandas/io/parsers.py:179(_read)\n",
        "       81    0.001    0.000  168.141    2.076 /usr/lib/python2.7/dist-packages/pandas/io/parsers.py:620(read)\n",
        "       81    0.002    0.000  167.610    2.069 /usr/lib/python2.7/dist-packages/pandas/io/parsers.py:948(read)\n",
        "       81  167.606    2.069  167.606    2.069 {method 'read' of 'pandas._parser.TextReader' objects}\n",
        "      163   10.516    0.065   10.516    0.065 {numpy.core.multiarray.concatenate}\n",
        "       81    0.001    0.000   10.258    0.127 /usr/lib/python2.7/dist-packages/pandas/core/frame.py:4300(append)\n",
        "       81    0.000    0.000   10.257    0.127 /usr/lib/python2.7/dist-packages/pandas/tools/merge.py:843(concat)\n",
        "        3    0.093    0.031    6.137    2.046 /usr/lib/python2.7/dist-packages/pandas/core/nanops.py:25(f)\n",
        "\n",
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<pstats.Stats instance at 0x327b680>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok so if we read the \"cumtime\" column  we can find that most of the time is actually spent on reading the data as a pandas data frame. \n",
      "There is also half a minute spent on merging, appending and concatanating files - which is ~15% of the runtime... we may be able to  assign different cores to join different data frames together.\n",
      "The reading of files is actually quite embarrassingly parallel but is also limited by the memory. \n",
      "Each file is only "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '{0:.0f} MB'.format( 22 / 81. * 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "272 MB\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even if we read 4 at the same time it should only be ~1 GB for my quad core desktop with 16 GB of RAM..."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "finding $\\alpha$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analyzing how much sequential code vs parallelizable code there is and trying to strive for complying with the Amdalh's law with my code!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallelize the bottleneck - different approaches within python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "multi-threading (processing) in python?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python has a global interpretted lock (GIL) which prevents more than one thread from being run at a time under usual circumstances (it's implemented like a mutex). \n",
      "Python users are advised to use multiple processes via the multiprocessing module <http://docs.python.org/dev/library/multiprocessing.html>\n",
      "that is part of the standard python library instead.\n",
      "The difference between the two is that memory aren't shared naturally between multiple processes in the case of multiple threads. \n",
      "Not sure if that is going to impact us here. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "parallel python packages "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are quite a number of powerful packages that can handle either parallelization on multiple core machine or over a distributed network of machines. \n",
      "<https://wiki.python.org/moin/ParallelProcessing>\n",
      "For simplicity I will use the standard library from python."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overall timings "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There should be a plot of overall timings "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Profiling of how long each part of the computations took and intrepretation "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I care about if the computation takes $O(N)$ or worse than that ($N$ being the number of lines in the files). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Speed up based on amount of data processed "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Speed up using multiple cores"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hyperthreading enables two threads to be run per core so I can test using up to 8 threads using my quad core machine - too bad they don't actually provide 8 times speed up but ~5.2 times (we are being too greedy). How hyperthreads scale would be limited by other hardware such as the memory bus. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verification of results "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Logistics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code dependencies "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to run the code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Machine Specifications"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Machine 1 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Corsair Vengeance 2x8GB DDR3 1600 MHz Desktop Memory,\n",
      "* Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz, (Quad core supports hyperthreading - 2 threads per core)\n",
      "<http://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-4770K+%40+3.50GHz&id=1919>\n",
      "* GeForce GTX 770 SuperClocked\n",
      "* Samsung 840 Pro 256 GB SSD\n",
      "* Western Digital HDD 2TB Intellipower adjustable RPM 5400 - 7200 \n",
      "* Motherboard: Asus Z87-Deluxe DDR3 1600 LGA 1150\n",
      "* Linux Mint 15 Olivia (GNU/Linux 3.8.0-19-generic x86_64)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Machine 2 ?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Actual code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "References:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] <http://cyrille.rossant.net/profiling-and-optimizing-python-code/>\n",
      "\n",
      "[2]"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}