{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stat 250 HW 2 with Prof. Duncan Temple Lang\n",
      "\n",
      "Author: (Karen) Yin-Yee Ng <karenyng@ucdavis.edu>\n",
      "\n",
      "Github repository: https://github.com/karenyng\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%autosave 60\n",
      "\n",
      "# the following two lines allows the generating \n",
      "# a floating table of content \n",
      "# currently does not work with nbviewer online nor pdf \n",
      "# I may fix it someday during my infinite spare time \n",
      "%load_ext nbtoc\n",
      "%nbtoc\n",
      "\n",
      "from __future__ import division\n",
      "%load_ext line_profiler\n",
      "%load_ext memory_profiler"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "javascript": [
        "IPython.notebook.set_autosave_interval(60000)"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Autosaving every 60 seconds\n",
        "The nbtoc extension is already loaded. To reload it, use:\n",
        "  %reload_ext nbtoc\n"
       ]
      },
      {
       "html": [
        "<!-- extracted from https://gist.github.com/magican/5574556 -->\n",
        "<div id=\"toc-wrapper\">\n",
        "    <div class=\"header\">Contents <a href=\"#\" class=\"hide-btn\">[hide]</a></div>\n",
        "    <div id=\"toc\"></div>\n",
        "</div>\n",
        " \n",
        "<style>\n",
        "  #toc {\n",
        "    overflow-y: scroll;\n",
        "    max-height: 300px;\n",
        "  }\n",
        "  #toc-wrapper {\n",
        "    position: fixed; top: 120px; max-width:430px; right: 20px;\n",
        "    border: thin solid rgba(0, 0, 0, 0.38); opacity: .8;\n",
        "    border-radius: 5px; background-color: #fff; padding:10px;\n",
        "    z-index: 100;\n",
        "  }\n",
        "  #toc-wrapper.closed {\n",
        "      min-width: 100px;\n",
        "      width: auto;\n",
        "      transition: width;\n",
        "  }\n",
        "  #toc-wrapper:hover{\n",
        "      opacity:1;\n",
        "  }\n",
        "  #toc-wrapper .header {\n",
        "      font-size:18px; font-weight: bold;\n",
        "  }\n",
        "  #toc-wrapper .hide-btn {\n",
        "      font-size: 14px;\n",
        "  }\n",
        " \n",
        "</style>\n",
        "\n",
        "<style>\n",
        "  ol.nested {\n",
        "    counter-reset: item;\n",
        "    list-style: none;\n",
        "  }\n",
        "  li.nested {\n",
        "        display: block;\n",
        "    }\n",
        "  li.nested:before {\n",
        "        counter-increment: item;\n",
        "        content: counters(item, \".\")\" \";\n",
        "    }\n",
        "</style>\n"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "javascript": [
        "// adapted from https://gist.github.com/magican/5574556\n",
        "\n",
        "function clone_anchor(element) {\n",
        "  // clone link\n",
        "  var h = element.find(\"div.text_cell_render\").find(':header').first();\n",
        "  var a = h.find('a').clone();\n",
        "  var new_a = $(\"<a>\");\n",
        "  new_a.attr(\"href\", a.attr(\"href\"));\n",
        "  // get the text *excluding* the link text, whatever it may be\n",
        "  var hclone = h.clone();\n",
        "  hclone.children().remove();\n",
        "  new_a.text(hclone.text());\n",
        "  return new_a;\n",
        "}\n",
        "\n",
        "function ol_depth(element) {\n",
        "  // get depth of nested ol\n",
        "  var d = 0;\n",
        "  while (element.prop(\"tagName\").toLowerCase() == 'ol') {\n",
        "    d += 1;\n",
        "    element = element.parent();\n",
        "  }\n",
        "  return d;\n",
        "}\n",
        "\n",
        "function table_of_contents(threshold) {\n",
        "  if (threshold === undefined) {\n",
        "    threshold = 4;\n",
        "  }\n",
        "  var cells = IPython.notebook.get_cells();\n",
        "  \n",
        "  var ol = $(\"<ol/>\");\n",
        "  $(\"#toc\").empty().append(ol);\n",
        "  \n",
        "  for (var i=0; i < cells.length; i++) {\n",
        "    var cell = cells[i];\n",
        "    \n",
        "    if (cell.cell_type !== 'heading') continue;\n",
        "    \n",
        "    var level = cell.level;\n",
        "    if (level > threshold) continue;\n",
        "    \n",
        "    var depth = ol_depth(ol);\n",
        "\n",
        "    // walk down levels\n",
        "    for (; depth < level; depth++) {\n",
        "      var new_ol = $(\"<ol/>\");\n",
        "      ol.append(new_ol);\n",
        "      ol = new_ol;\n",
        "    }\n",
        "    // walk up levels\n",
        "    for (; depth > level; depth--) {\n",
        "      ol = ol.parent();\n",
        "    }\n",
        "    //\n",
        "    ol.append(\n",
        "      $(\"<li/>\").append(clone_anchor(cell.element))\n",
        "    );\n",
        "  }\n",
        "\n",
        "  $('#toc-wrapper .header').click(function(){\n",
        "    $('#toc').slideToggle();\n",
        "    $('#toc-wrapper').toggleClass('closed');\n",
        "    if ($('#toc-wrapper').hasClass('closed')){\n",
        "      $('#toc-wrapper .hide-btn').text('[show]');\n",
        "    } else {\n",
        "      $('#toc-wrapper .hide-btn').text('[hide]');\n",
        "    }\n",
        "    return false;\n",
        "  })\n",
        "\n",
        "  $(window).resize(function(){\n",
        "    $('#toc').css({maxHeight: $(window).height() - 200})\n",
        "  })\n",
        "\n",
        "  $(window).trigger('resize')\n",
        "}\n",
        "\n",
        "table_of_contents();\n",
        "\n",
        "\n"
       ],
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The line_profiler extension is already loaded. To reload it, use:\n",
        "  %reload_ext line_profiler\n",
        "The memory_profiler extension is already loaded. To reload it, use:\n",
        "  %reload_ext memory_profiler\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Background of this assignment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We parallelize previous methods  for computing statistics of large csv methods with goals of: \n",
      "\n",
      "* comparing different ways of parallelizing the code\n",
      "* examining the speeding up of the code from different ways of parallelism\n",
      "\n",
      "Discussion of the previous methods is available at: \n",
      "\n",
      "<http://nbviewer.ipython.org/github/karenyng/HW1_Stat250_Winter14/blob/master/writeup/hw1.ipynb?create=1>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Possible difficulties / overhead"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* handling data locality - avoid passing data around different workers \n",
      "* overhead of combining data to compute the median"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notes for myself - Key concepts of parallelism "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "> Data parallelism\n",
      "\n",
      "handling multiple inputs concurrently\n",
      "\n",
      "> Task parallelism\n",
      "\n",
      "if tasks are not dependent on each other, then they can be perform simultaneously with different workers\n",
      "\n",
      "> Data locality \n",
      "\n",
      "Be careful about how to fork the data.  Fine grained functions is hard to parallelize well since a lot of communication about the data will decrease the efficiency.\n",
      "Also have to be aware of how the language scope the functions / variables when sending them from master node to worker nodes\n",
      "\n",
      "> Race condition \n",
      "\n",
      "> Deadlock \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "My idea of how the most efficient algorithm in terms of both speed and memory usage should go: "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "use a compiled language:\n",
      "\n",
      "* use threads to read in column data of each file concurrently line by line into shared memory (1st pass) while making a frequency table for the column data for each file\n",
      "* combine frequency tables (this has to done be sequentially)\n",
      "\n",
      "* make two more copies of the combined frequency table assuming the table is small enough that copying is fast\n",
      "* compute mean, median and standard deviation from the 3 copies of the frequency tables with three threads"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Intrinsic limitations - ideally how well can we do?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Amdalh's law  (reference from wikipedia...)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ lim_{P-> \\infty} \\frac{1}{\\frac{1-\\alpha}{P} + \\alpha} = \\frac{1}{\\alpha} $$\n",
      "\n",
      "which denotes the maximum speed up with parallelization of the program, where $\\alpha$ is the portion of the sequential parts of the program that is not speed up with addition of processors."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Gustafson's law "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$ S(P) = \\alpha + P(1-\\alpha) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So during profiling I should find $\\alpha$ in order to know how well I can do."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Performance metric of how well I am parallelizing my code:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "(From Scaling Up Machine Learning - Bekkerman R.) \n",
      "\n",
      "* speedup - ratio of solution time for sequential algorithms vs. its parallel counterpart\n",
      "\n",
      "* efficiency - measures the ratio of speed up to the # of processors\n",
      "\n",
      "* scalability - tracks efficiency as a function of an increasing number of processors  <- how are the lower two different!?!?!?!?!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Method 1: Parallelization in python "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "My code originally runs for ~3 mins. I am parallelizing it because I want to learn how to parallelize python code properly, most of the codes between me and my collaborators are in python (it's a community preference not a personal one). \n",
      "\n",
      "\n",
      "I mainly make use of the book \"Python high performance programming\" as my reference for this method."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Profiling my python code to see where the speed / memory bottleneck is"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Following a really nice tutorial at: \n",
      "\n",
      "<http://pynash.org/2013/03/06/timing-and-profiling.html>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "import a version of the code that is wrapped as a function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from profile_stat import run"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "run line profiler on it and dump the output to lpstat.txt"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    %prun -T lpstat.txt run()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Educated guess: the reading in of files would take the most amount of time ... "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -10 lpstat.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         50247 function calls (50245 primitive calls) in 190.815 seconds\r\n",
        "\r\n",
        "   Ordered by: internal time\r\n",
        "\r\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n",
        "       81  167.560    2.069  167.560    2.069 {method 'read' of 'pandas._parser.TextReader' objects}\r\n",
        "      163   10.553    0.065   10.553    0.065 {numpy.core.multiarray.concatenate}\r\n",
        "        1    2.619    2.619    2.619    2.619 {pandas.algos.median}\r\n",
        "        1    1.483    1.483  190.694  190.694 profile_stat.py:8(run)\r\n",
        "        4    1.387    0.347    1.387    0.347 common.py:128(_isnull_ndarraylike)\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok so if we read the \"cumtime\" column  we can find that most of the time is actually spent on reading the data the csv files. \n",
      "There is also half a minute spent on merging, appending and concatanating files - which is ~15% of the runtime... we may be able to  assign different cores to join different data frames together.\n",
      "The reading of files is actually quite embarrassingly parallel but is also limited by the memory. \n",
      "Each file is only "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print '{0:.0f} MB'.format(2.2 / 81. * 1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "27 MB\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even if we read 4 at the same time it should only be ~0.1 GB for my quad core desktop with 16 GB of RAM... unless my code is doing something really stupid.\n",
      "\n",
      "Let's do memory profiling to see if that's true!\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    %mprun -T mpstat.txt -f run run() "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!cat mpstat.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Filename: profile_stat.py\r\n",
        "\r\n",
        "Line #    Mem usage    Increment   Line Contents\r\n",
        "================================================\r\n",
        "     8    132.0 MiB      0.0 MiB   def run:\r\n",
        "     9    132.0 MiB      0.0 MiB       import pandas as pd\r\n",
        "    10    132.0 MiB      0.0 MiB       import numpy as np\r\n",
        "    11    132.0 MiB      0.0 MiB       data_path = \"../data/\"\r\n",
        "    12                             \r\n",
        "    13                                 # First read in the data from 1987 to 2007\r\n",
        "    14    132.0 MiB      0.0 MiB       year = [data_path + str(i) + \".csv\" for i in range(1987, 2008)]\r\n",
        "    15                                 # create empty dataframe\r\n",
        "    16    132.0 MiB      0.0 MiB       delay1 = pd.DataFrame()\r\n",
        "    17                                 # loop through the year-by-year csvs\r\n",
        "    18   1993.0 MiB   1861.0 MiB       for yr_file in year:\r\n",
        "    19                                     # read in relevant column from csv file using pandas\r\n",
        "    20   1879.3 MiB   -113.7 MiB           temp = pd.read_csv(yr_file, usecols=[\"ArrDelay\"])\r\n",
        "    21                                     # append the dataframes - this is done by reference not by value\r\n",
        "    22   1993.0 MiB    113.7 MiB           delay1 = delay1.append(temp)\r\n",
        "    23   1993.0 MiB      0.0 MiB           print 'appending ' + yr_file + ' - total lines = ' + \\\r\n",
        "    24   1993.0 MiB      0.0 MiB               '{0}'.format(delay1.shape[0])\r\n",
        "    25                             \r\n",
        "    26                                 # create another empty dataframe for handling month by month csv\r\n",
        "    27   1993.0 MiB      0.0 MiB       delay2 = pd.DataFrame()\r\n",
        "    28   1993.0 MiB      0.0 MiB       month = ['January', 'February', 'March', 'April', 'May', 'June', 'July',\r\n",
        "    29   1993.0 MiB      0.0 MiB               'August', 'September', 'October', 'November', 'December']\r\n",
        "    30   1993.0 MiB      0.0 MiB       year = [data_path + str(i) + \"_\" +\r\n",
        "    31   1993.0 MiB      0.0 MiB               mth + \".csv\" for i in range(2008, 2013) for mth in month"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "]\r\n",
        "    32                                 # loop through all the month-by-month csv\r\n",
        "    33   2399.7 MiB    406.7 MiB       for yr_file in year:\r\n",
        "    34                                     # tell pandas to read only the relevant column in the csv\r\n",
        "    35   2392.2 MiB     -7.5 MiB           temp = pd.read_csv(yr_file, usecols=[\"ARR_DELAY\"])\r\n",
        "    36                                     # append them to the dataframe by reference\r\n",
        "    37   2399.7 MiB      7.5 MiB           delay2 = delay2.append(temp)\r\n",
        "    38   2399.7 MiB      0.0 MiB           print 'appending ' + yr_file + ' - total lines = ' + \\\r\n",
        "    39   2399.7 MiB      0.0 MiB               '{0}'.format(delay2.shape[0] + delay1.shape[0])\r\n",
        "    40                             \r\n",
        "    41                                 # hackish way to remove the column name of the dataframe to append\r\n",
        "    42                                 # the two types of csv columns together\r\n",
        "    43                                 # so I can compute the statistics in one pass later on\r\n",
        "    44   1510.7 MiB   -889.0 MiB       delay1 = np.array(delay1)\r\n",
        "    45   1265.8 MiB   -244.8 MiB       delay2 = np.array(delay2)\r\n",
        "    46   2399.7 MiB   1133.9 MiB       delay = np.append(delay1, delay2)\r\n",
        "    47   3533.6 MiB   1133.9 MiB       delay = pd.DataFrame(delay)\r\n",
        "    48   3533.6 MiB      0.0 MiB       print 'total number of valid lines = {0}'.format(delay.dropna().shape[0])\r\n",
        "    49                             \r\n",
        "    50                                 # note that pandas ignores nans automatically while computing stats\r\n",
        "    51   3533.6 MiB      0.0 MiB       print 'saving to results2.txt'\r\n",
        "    52   3533.6 MiB      0.0 MiB       f = open('results2.txt', 'w')\r\n",
        "    53   3533.6 MiB      0.0 MiB       f.write('mean = {0}\\n'.format(delay[0].mean()))\r\n",
        "    54   3533.6 MiB      0.0 MiB       f.write('median = {0}\\n'.format(delay[0].median()))\r\n",
        "    55   3533.6 MiB      0.0 MiB       f.write('std = {0}\\n'.format(delay[0].std()))\r\n",
        "    56   3533.6 MiB     -0.0 MiB       f.close()"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the python memory profiler was reporting back that only ~3.5 GB of memory was used.\n",
      "Not sure if that is really true since the C backend of pandas might be by-passing the python intrepretter for memory usage. So I used valgrind to measure the usage. The memory use is consistent with what the python profiler found. \n",
      "The valgrind profiling results are stored in ${GIT REPOSITORY}/analysis/profile_m1_mem.txt. My code is NOT memory bound."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "finding $\\alpha$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analyzing how much sequential code vs parallelizable code there is and trying to strive for complying with the Amdalh's law with my code!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Parallelize the bottleneck - different approaches within python"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "multi-threading (processing) in python?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python has a global interpretted lock (GIL) which prevents more than one thread from being run at a time under usual circumstances (it's implemented like a mutex). \n",
      "Python users are advised to use multiple processes via the multiprocessing module \n",
      "\n",
      "<http://docs.python.org/dev/library/multiprocessing.html>\n",
      "\n",
      "that is part of the standard python library instead.\n",
      "The difference between the two is that memory aren't shared naturally between multiple processes in the case of multiple threads. \n",
      "Not sure if that is going to impact us here. \n",
      "\n",
      "I tried using the default multiprocessing module but it does not like functions with optional arguments.\n",
      "Writing a wrapper function to parse the optional arguments also did not work properly. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "parallel python packages "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are quite a number of powerful packages that can handle either parallelization on multiple core machine or over a distributed network of machines. \n",
      "\n",
      "<https://wiki.python.org/moin/ParallelProcessing>\n",
      "\n",
      "In this assignment, I use the \"parallel python\" package (pp). The package seems quite comprehensive, it supports multicore processing, distributed computing over a cluster etc.\n",
      "But after some use, I do not actually like this package, it has the worst (close to non-existent) python package documentation I have seen. \n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Timing and memory profile of parallelized code using the parallel python package"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    from profile_method1 import run\n",
      "    %prun -T lpstat_parallelized_m1.txt run()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -15 lpstat_parallelized_m1.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         24731 function calls (24730 primitive calls) in 244.903 seconds\r\n",
        "\r\n",
        "   Ordered by: internal time\r\n",
        "\r\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n",
        "      450  223.675    0.497  223.675    0.497 {method 'acquire' of 'thread.lock' objects}\r\n",
        "      164   11.006    0.067   11.006    0.067 {numpy.core.multiarray.concatenate}\r\n",
        "        1    2.620    2.620    2.620    2.620 {pandas.algos.median}\r\n",
        "        1    1.201    1.201  244.566  244.566 profile_method1.py:1(run)\r\n",
        "       81    1.163    0.014    1.519    0.019 {cPickle.loads}\r\n",
        "        3    1.034    0.345    1.034    0.345 common.py:128(_isnull_ndarraylike)\r\n",
        "        2    0.711    0.355    0.711    0.355 {method 'copy' of 'numpy.ndarray' objects}\r\n",
        "      253    0.584    0.002    0.584    0.002 {method 'reduce' of 'numpy.ufunc' objects}\r\n",
        "        1    0.561    0.561    0.561    0.561 {open}\r\n",
        "        1    0.510    0.510    3.575    3.575 nanops.py:125(get_median)\r\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The rest of the file does not have any more process that takes up a big chunk of computing time.\n",
      "So the Python Global Interpreter Lock (GIL) is really messing things up here."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "    from profile_method1a import run\n",
      "    %prun -T lpstat_parallelized_m1a.txt run()"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!head -15 lpstat_parallelized_m1a.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "         4673 function calls in 249.731 seconds\r\n",
        "\r\n",
        "   Ordered by: internal time\r\n",
        "\r\n",
        "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n",
        "      494  161.209    0.326  161.209    0.326 {method 'acquire' of 'thread.lock' objects}\r\n",
        "       81   44.126    0.545  155.997    1.926 threading.py:726(start)\r\n",
        "       81   22.865    0.282   22.865    0.282 {numpy.core.multiarray.concatenate}\r\n",
        "        1    5.846    5.846  249.630  249.630 profile_method1a.py:22(run)\r\n",
        "        1    5.201    5.201    5.201    5.201 {method 'sort' of 'numpy.ndarray' objects}\r\n",
        "       83    2.615    0.032  164.246    1.979 threading.py:308(wait)\r\n",
        "       81    2.533    0.031    2.785    0.034 threading.py:560(__init__)\r\n",
        "       81    1.480    0.018    1.480    0.018 {method 'ravel' of 'numpy.ndarray' objects}\r\n",
        "      162    0.691    0.004    0.691    0.004 threading.py:259(__init__)\r\n",
        "       81    0.546    0.007    0.546    0.007 threading.py:627(_newname)\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is very frustrating to work python threads due to the global intrepreter lock."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Overall timings "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok let's parse back the output of the \"time\" command from the shell. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "df = pd.read_table(\"./runtime/time_cpus.txt\", names = [\"time_type\", \"time\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = \"5m6.609s\"\n",
      "(min, sec) = test.split(\"m\")\n",
      "sec = sec[:5]\n",
      "\n",
      "def get_substring(string):\n",
      "    return string[:5] "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df[\"time\"].applymap("
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.apply(axis=1, func = split, args=(\"m\",))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "Axis must be 0 or 1, got time",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-44-b92d9fe437e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"m\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, args, **kwds)\u001b[0m\n\u001b[0;32m   4154\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4155\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4156\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4157\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4158\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures)\u001b[0m\n\u001b[0;32m   4199\u001b[0m                           enumerate(izip(values, res_index)))\n\u001b[0;32m   4200\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4201\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Axis must be 0 or 1, got %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4203\u001b[0m         \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: Axis must be 0 or 1, got time"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Profiling of how long each part of the computations took and intrepretation "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I care about if the computation takes $O(N)$ or worse than that ($N$ being the number of lines in the files). "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Speed up based on amount of data processed "
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Speed up using multiple cores"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "plot of speed up vs # of cores"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Verification of results "
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Method 2 ---- Pure C"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Possibly the slowest steps in terms of development time to complete the assignment"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* gather pieces of Duncan 's Arr Delay package code and decide which ones to adopt\n",
      "* unit test the adopted code\n",
      "* run valgrind to make sure there is no memory leak!\n",
      "* put them together\n",
      "* run the sequential version first to make sure everything work???\n",
      "* run the threaded version\n",
      "* do global test...\n",
      "* find a time-profiler for profiling each part of the code\n",
      "\n",
      "What I need to remind myself NOT to do:\n",
      "\n",
      "* don't spend time writing a makefile\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Logistics"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Code dependencies "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "How to run the code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Machine Specifications"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Machine 1 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Corsair Vengeance 2x8GB DDR3 1600 MHz Desktop Memory,\n",
      "* Intel(R) Core(TM) i7-4770K CPU @ 3.50GHz\n",
      "<http://www.cpubenchmark.net/cpu.php?cpu=Intel+Core+i7-4770K+%40+3.50GHz&id=1919>\n",
      "* GeForce GTX 770 SuperClocked\n",
      "* Samsung 840 Pro 256 GB SSD\n",
      "* Western Digital HDD 2TB Intellipower adjustable RPM 5400 - 7200 \n",
      "* Motherboard: Asus Z87-Deluxe DDR3 1600 LGA 1150\n",
      "* Linux Mint 15 Olivia (GNU/Linux 3.8.0-19-generic x86_64)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Software package dependencies on machine 1 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python package dependencies: \n",
      "\n",
      "* numpy 1.7.1\n",
      "* pandas 0.10.1\n",
      "* pp 1.6.4\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Machine 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Mac Pro 5.1 \n",
      "* Processor Name: Intel 6-Core Xeon @ 2.66 GHz \n",
      "* Number of Processors: 2\n",
      "* Total Number of Cores: 12\n",
      "* L2 Cache (per Core): 256 KB\n",
      "* L3 Cache (per Processor): 12 MB\n",
      "* Memory: 24 GB\n",
      "* Processor Interconnect Speed: 6.4 GT/s\n",
      "* OSX Maverick 10.9.1"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Software package dependencies on machine 2 "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python package dependencies: \n",
      "\n",
      "* numpy 1.8.0\n",
      "* pandas 0.13.0\n",
      "* pp 1.6.4 (not the best python package!!!)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Actual code"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "References:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[1] <http://cyrille.rossant.net/profiling-and-optimizing-python-code/>\n",
      "\n",
      "[2]"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}